{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikhlas15/ATHENS-AI-Medical-Imaging/blob/main/H12_transfer_learning_medical_imaging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b9effaf-f5cc-4e19-871c-701597029aef",
      "metadata": {
        "id": "7b9effaf-f5cc-4e19-871c-701597029aef"
      },
      "source": [
        "# **Notebook 12: Transfer Learning for Medical Image Classification**\n",
        "\n",
        "### **Course**: Artificial Intelligence in Medical Imaging: From Fundamentals to Applications\n",
        "\n",
        "***\n",
        "\n",
        "## **1. Introduction**\n",
        "\n",
        "Welcome to Notebook 12! So far, we have trained our own CNNs from scratch. While this is a great way to learn, it can be inefficient, especially when working with small medical datasets. Today, we will explore one of the most powerful and widely used techniques in deep learning: **Transfer Learning**.\n",
        "\n",
        "Transfer learning is the process of taking a model that has been pre-trained on a very large dataset (like ImageNet, which contains millions of everyday images) and adapting it for a new, specific task (like detecting pneumonia in chest X-rays). The core idea is that the features learned on the large dataset—such as edges, textures, shapes, and object parts—are often general enough to be useful for our new task.\n",
        "\n",
        "#### **What you will learn today:**\n",
        "*   The motivation and benefits of using transfer learning in medical imaging AI.\n",
        "*   How to load a state-of-the-art model (ResNet18) pre-trained on ImageNet.\n",
        "*   How to adapt the pre-trained model for our specific medical imaging task.\n",
        "*   The two-phase fine-tuning strategy:\n",
        "    1.  **Feature Extraction:** Freezing the pre-trained layers and training only the new classification head.\n",
        "    2.  **Fine-Tuning:** Unfreezing the entire network and continuing to train with a small learning rate.\n",
        "*   How to handle common challenges, such as mismatched input channels (grayscale vs. RGB) and image sizes.\n",
        "\n",
        "***\n",
        "\n",
        "## **2. Setup: Installing and Importing Libraries**\n",
        "\n",
        "Let's begin by preparing our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49f78ed6-6567-4213-94aa-386dd311cbcc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49f78ed6-6567-4213-94aa-386dd311cbcc",
        "outputId": "bba51d57-c662-40d1-8721-320b8a56d9d7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Collecting medmnist\n",
            "  Downloading medmnist-3.0.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from medmnist) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from medmnist) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from medmnist) (0.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from medmnist) (4.67.1)\n",
            "Collecting fire (from medmnist)\n",
            "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire->medmnist) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (1.16.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (2025.10.4)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->medmnist) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->medmnist) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->medmnist) (1.17.0)\n",
            "Downloading medmnist-3.0.2-py3-none-any.whl (25 kB)\n",
            "Downloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fire, medmnist\n",
            "Successfully installed fire-0.7.1 medmnist-3.0.2\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision medmnist\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# Import Dataloader from Pytorch\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#TODO import PneumoniaMNIST dataset from medmnist\n",
        "\n",
        "\n",
        "# Set our standard random seed and device\n",
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {...}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad3d2cc-7f5e-4c9d-877d-ea4471819f92",
      "metadata": {
        "id": "9ad3d2cc-7f5e-4c9d-877d-ea4471819f92"
      },
      "source": [
        "***\n",
        "\n",
        "## **3. Preparing Data for Transfer Learning**\n",
        "\n",
        "This is a critical step. Pre-trained models have specific expectations for their input data because they were trained in a particular way. To use a model pre-trained on ImageNet, we must preprocess our medical images to match the ImageNet format as closely as possible.\n",
        "\n",
        "This involves three key transformations:\n",
        "1.  **Resize Images:** ImageNet models are typically trained on 224x224 images. We must resize our smaller 28x28 PneumoniaMNIST images to this size.\n",
        "2.  **Handle Input Channels:** Our X-ray images are grayscale (1 channel), but ImageNet models expect 3-channel RGB images. The simplest solution is to duplicate the single grayscale channel three times.\n",
        "3.  **Normalize with ImageNet Statistics:** We must normalize our images using the exact `mean` and `standard deviation` that were used to train the original model. For ImageNet, these are standard, well-known values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7777ba66-fd17-405a-830f-2511efe64ba3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7777ba66-fd17-405a-830f-2511efe64ba3",
        "outputId": "05c78b92-1fb4-468c-b513-4a4b05dbb566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.17M/4.17M [00:01<00:00, 3.47MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 4708 images.\n",
            "Validating on 524 images.\n"
          ]
        }
      ],
      "source": [
        "# Define the special transformations for transfer learning\n",
        "# Hint: Use transforms.Compose([...]) to stack multiple operations\n",
        "transfer_learning_transforms = ...([\n",
        "    transforms.Resize((224, 224)),\n",
        "\n",
        "    # Hint: Convert 1-channel images to 3 channels for ResNet\n",
        "    transforms.Grayscale(num_output_channels=____),\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # Hint: Normalize using ImageNet mean and std lists\n",
        "    transforms. ... (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "# Load the datasets with these new transforms\n",
        "# Hint: Fill \"train\" or \"val\" for the dataset split\n",
        "train_dataset = PneumoniaMNIST(split='______',\n",
        "                               transform=transfer_learning_transforms,\n",
        "                               download=True)\n",
        "\n",
        "val_dataset = PneumoniaMNIST(split='_____',\n",
        "                             transform=...,\n",
        "                             download=True)\n",
        "\n",
        "\n",
        "# Create DataLoaders\n",
        "# Hint: Common batch size is 32, shuffle only the training set\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=____, shuffle=______)\n",
        "val_loader   = DataLoader(dataset=val_dataset, batch_size=____, shuffle=______)\n",
        "\n",
        "\n",
        "print(f\"Training on {len(train_dataset)} images.\")\n",
        "print(f\"Validating on {len(val_dataset)} images.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c35c28-59bb-4363-80bf-b4ad6ff8f83b",
      "metadata": {
        "id": "a1c35c28-59bb-4363-80bf-b4ad6ff8f83b"
      },
      "source": [
        "***\n",
        "\n",
        "## **4. Loading and Adapting the Pre-trained Model**\n",
        "\n",
        "We will use **ResNet18**, a popular and efficient architecture, pre-trained on ImageNet.\n",
        "\n",
        "### **4.1. Loading the Model**\n",
        "We load the model using `torchvision.models`, setting `weights=ResNet18_Weights.DEFAULT` to download the learned weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8455d7c-3536-4152-8c91-3cc42c28cb30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8455d7c-3536-4152-8c91-3cc42c28cb30",
        "outputId": "8e6837ff-cc72-4509-f0e0-e2ea701d4f19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 169MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Load the pretrained ResNet18 model\n",
        "model = models....(weights=\"ResNet18_Weights.DEFAULT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5dcae8a-9aa8-47b8-a7a0-d42fe7a6661c",
      "metadata": {
        "id": "b5dcae8a-9aa8-47b8-a7a0-d42fe7a6661c"
      },
      "source": [
        "### **4.2. Adapting the Model**\n",
        "The loaded model is designed for ImageNet. We need to make two changes:\n",
        "1.  (Optional but good practice) We already handled the input channel mismatch in our data transforms, so we don't need to change the first convolutional layer.\n",
        "2.  **Replace the Classifier Head:** The final layer of ResNet18, `model.fc`, is a `Linear` layer that outputs 1000 values (for the 1000 ImageNet classes). We must replace this with a new `Linear` layer that outputs 2 values for our binary classification task (Normal vs. Pneumonia).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dd9eecb-0ff0-4f61-8a1a-08a2fe2fc375",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dd9eecb-0ff0-4f61-8a1a-08a2fe2fc375",
        "outputId": "ed60e181-9e1e-4520-ff77-6bb0e9077d0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model architecture adapted for our task ---\n"
          ]
        }
      ],
      "source": [
        "# Get the number of input features for the classifier\n",
        "num_ftrs = model.fc. ...\n",
        "\n",
        "# Replace the final fully connected layer with a new one for our task\n",
        "model.fc = nn.Linear(..., 2)\n",
        "\n",
        "# Move the model to the GPU\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"--- Model architecture adapted for our task ---\")\n",
        "# print(model) # Uncomment to see the full architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d451d05-851b-4230-a69f-b9f36d0dac95",
      "metadata": {
        "id": "5d451d05-851b-4230-a69f-b9f36d0dac95"
      },
      "source": [
        "***\n",
        "\n",
        "## **5. Fine-Tuning Strategy: Phase 1 (Train the Head Only)**\n",
        "\n",
        "Our model now consists of a frozen, pre-trained \"body\" (the convolutional layers) and a new, randomly initialized \"head\" (our `Linear` layer). If we start training the whole network immediately, the large, random gradients from the untrained head could corrupt the finely-tuned weights of the body.\n",
        "\n",
        "The best practice is to first **freeze the body and train only the head**.\n",
        "\n",
        "### **5.1. Freezing the Pre-trained Layers**\n",
        "We loop through all the model's parameters and set their `requires_grad` attribute to `False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7a42e19-2a06-43b4-9965-1cb24b7a14a3",
      "metadata": {
        "id": "a7a42e19-2a06-43b4-9965-1cb24b7a14a3"
      },
      "outputs": [],
      "source": [
        "# Freeze all the parameters in the model\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = ...\n",
        "\n",
        "# Unfreeze ONLY the parameters of the new final layer\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4af51ad8-94d1-4c66-82e8-8b63b507d327",
      "metadata": {
        "id": "4af51ad8-94d1-4c66-82e8-8b63b507d327"
      },
      "source": [
        "### **5.2. Training the Head**\n",
        "Now, we create an optimizer that will *only* update the parameters where `requires_grad` is `True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9be432d-08a5-46cf-94d6-85b0d5aa911c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9be432d-08a5-46cf-94d6-85b0d5aa911c",
        "outputId": "6a5fccae-7939-4094-b8ad-6fc62df7187e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Phase 1: Training the classifier head ---\n",
            "Epoch [1/3] - Train Loss: 0.3450, Train Acc: 0.8549\n",
            "Epoch [2/3] - Train Loss: 0.2334, Train Acc: 0.9034\n",
            "Epoch [3/3] - Train Loss: 0.2080, Train Acc: 0.9153\n"
          ]
        }
      ],
      "source": [
        "# Create an optimizer that only updates the parameters of the new classifier\n",
        "# Hint: Use optim.Adam and filter only parameters with requires_grad=True\n",
        "optimizer = optim. ...(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
        "# Hint: Use the standard classification loss for multi-class problems\n",
        "criterion = nn. ...\n",
        "\n",
        "# Re-usable training function from previous notebooks\n",
        "def train_one_epoch(model, data_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss, total_correct = 0.0, 0\n",
        "    for images, labels in data_loader:\n",
        "         #Hint: Move both images and labels to the correct device\n",
        "        images, labels = images. ... , labels.... .squeeze().long()\n",
        "         # Hint: reset gradients\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Hint: compute gradients\n",
        "        loss.\n",
        "        # Hint: update parameters\n",
        "        optimizer.\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        total_correct += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
        "    return total_loss / len(data_loader.dataset), total_correct / len(data_loader.dataset)\n",
        "\n",
        "# Train for a few epochs\n",
        "print(\"--- Phase 1: Training the classifier head ---\")\n",
        "num_epochs_phase1 =  # Set a number (e.g 3)\n",
        "for epoch in range(num_epochs_phase1):\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs_phase1}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1e2a7ef-48f8-4f78-b905-d7b2c78a3999",
      "metadata": {
        "id": "f1e2a7ef-48f8-4f78-b905-d7b2c78a3999"
      },
      "source": [
        "***\n",
        "\n",
        "## **6. Fine-Tuning Strategy: Phase 2 (Unfreeze and Train All Layers)**\n",
        "\n",
        "Now that our new head is trained and stable, we can **unfreeze the entire network** and continue training. This will allow the pre-trained feature extractor to slightly adjust its weights to better suit our specific medical dataset.\n",
        "\n",
        "It is crucial to use a **very small learning rate** during this phase to avoid making drastic changes that would destroy the valuable pre-trained features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86dd6671-f96c-40f3-b2e0-a7e9ff0c0490",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86dd6671-f96c-40f3-b2e0-a7e9ff0c0490",
        "outputId": "bc655db1-63fc-4431-8394-c53d019e285d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Phase 2: Fine-tuning the entire model ---\n",
            "Epoch [1/5] - Train Loss: 0.1539, Train Acc: 0.9382\n",
            "Epoch [2/5] - Train Loss: 0.0742, Train Acc: 0.9794\n",
            "Epoch [3/5] - Train Loss: 0.0433, Train Acc: 0.9892\n",
            "Epoch [4/5] - Train Loss: 0.0283, Train Acc: 0.9938\n",
            "Epoch [5/5] - Train Loss: 0.0165, Train Acc: 0.9975\n"
          ]
        }
      ],
      "source": [
        "# Unfreeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad =\n",
        "\n",
        "# Create a new optimizer for the whole model with a very low learning rate\n",
        "optimizer = optim.Adam(...., lr=1e-5) # A much smaller learning rate\n",
        "\n",
        "# Continue training for a few more epochs\n",
        "print(\"\\n--- Phase 2: Fine-tuning the entire model ---\")\n",
        "num_epochs_phase2 = 5\n",
        "for epoch in range(...):\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs_phase2}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dacb73e8-0259-4631-bc2c-0326680c5339",
      "metadata": {
        "id": "dacb73e8-0259-4631-bc2c-0326680c5339"
      },
      "source": [
        "***\n",
        "\n",
        "## **7. Final Evaluation**\n",
        "\n",
        "Finally, let's evaluate our fully fine-tuned model on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fadc2eff-66de-4a52-896b-e643fb4ec5e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fadc2eff-66de-4a52-896b-e643fb4ec5e1",
        "outputId": "75f5b72b-6392-4f00-bf03-44bc52de4329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Validation Accuracy: 0.9637\n"
          ]
        }
      ],
      "source": [
        "# Re-usable evaluate function\n",
        "def evaluate(model, data_loader, device):\n",
        "  # Hint: Put the model in evaluation mode\n",
        "    model.\n",
        "    total_correct = 0\n",
        "    # Hint: Disable gradient computation during evaluation\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device).squeeze().long()\n",
        "            # Hint: do feed fo\n",
        "            outputs = model(images)\n",
        "            # Hint: Use torch.argmax(...) to get predicted class indices\n",
        "            total_correct += (torch. ...(outputs, dim=1) == labels).sum().item()\n",
        "    # Hint: accuracy = correct_predictions / total_samples\n",
        "    return total_correct / len(data_loader. ...)\n",
        "\n",
        "# Run validation\n",
        "val_accuracy = ...(model, val_loader, device)\n",
        "print(f\"\\nFinal Validation Accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4488636-fbb7-457f-b4d4-5b68afd0e3e3",
      "metadata": {
        "id": "f4488636-fbb7-457f-b4d4-5b68afd0e3e3"
      },
      "source": [
        "You should see a very high accuracy, likely much higher than what our simple baseline CNN from scratch could achieve, and with far less training time!\n",
        "\n",
        "***\n",
        "\n",
        "## **8. Summary and Next Steps**\n",
        "\n",
        "\n",
        "In the final notebook of this course, **`13_explainability_interpretability.ipynb`**, we will explore techniques to \"look inside the black box\" of our trained models to understand *why* they are making certain predictions, a critical step for building trust and deploying AI in clinical settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QLOi-g1V-ugA"
      },
      "id": "QLOi-g1V-ugA",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}