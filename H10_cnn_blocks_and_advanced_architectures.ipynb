{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikhlas15/ATHENS-AI-Medical-Imaging/blob/main/H10_cnn_blocks_and_advanced_architectures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92c00f63-c030-43d9-a1b8-2f69a4af8c86",
      "metadata": {
        "id": "92c00f63-c030-43d9-a1b8-2f69a4af8c86"
      },
      "source": [
        "# **Notebook 10: Advanced CNN Architectures (ResNet, U-Net)**\n",
        "\n",
        "### **Course**: Artificial Intelligence in Medical Imaging: From Fundamentals to Applications\n",
        "\n",
        "***\n",
        "\n",
        "## **1. Introduction**\n",
        "\n",
        "Welcome to Notebook 10! You've successfully built and trained a baseline CNN. Now, we will explore the architectural innovations that have enabled deep learning models to become incredibly deep and powerful. We will move beyond simple, sequential models and into the world of state-of-the-art network design.\n",
        "\n",
        "The architectures we discuss today, **ResNet** and **U-Net**, are arguably the two most important and influential designs in modern medical imaging AI.\n",
        "\n",
        "#### **What you will learn today:**\n",
        "*   The motivation behind **Residual Connections** and how they solve the \"vanishing gradient\" problem, allowing for extremely deep networks.\n",
        "*   How to build a **Residual Block**, the fundamental component of a ResNet.\n",
        "*   An overview of the complete **ResNet** architecture and how to use a pre-built version from `torchvision`.\n",
        "*   An introduction to the **U-Net** architecture, the gold standard for image segmentation tasks in medical imaging, and its famous \"encoder-decoder\" structure with \"skip connections.\"\n",
        "\n",
        "***\n",
        "\n",
        "## **2. Setup: Installing and Importing Libraries**\n",
        "\n",
        "Let's start with our usual setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5ab0f12-fa50-4857-8caf-2356f352dfc0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5ab0f12-fa50-4857-8caf-2356f352dfc0",
        "outputId": "1d30b392-71e5-41a3-a1e9-6571ff9ad9e0",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Collecting medmnist\n",
            "  Downloading medmnist-3.0.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from medmnist) (2.2.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from medmnist) (0.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from medmnist) (4.67.1)\n",
            "Collecting fire (from medmnist)\n",
            "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.12/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire->medmnist) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (2025.10.4)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "Downloading medmnist-3.0.2-py3-none-any.whl (25 kB)\n",
            "Downloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fire, medmnist\n",
            "Successfully installed fire-0.7.1 medmnist-3.0.2\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision medmnist scikit-learn seaborn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# TODO: Import the functional API from torch.nn\n",
        "import torch.nn. ... as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from medmnist import PneumoniaMNIST\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set our standard random seed and device\n",
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78cf2d18-6ca7-4706-becc-adac60408369",
      "metadata": {
        "id": "78cf2d18-6ca7-4706-becc-adac60408369"
      },
      "source": [
        "***\n",
        "\n",
        "## **3. The Challenge of Deep Networks and the Rise of Residual Connections**\n",
        "\n",
        "In theory, a deeper neural network should be more powerful because it can learn more complex and hierarchical features. However, in the early days of deep learning, researchers found that simply stacking more and more layers on top of each other led to a problem: **performance got worse**.\n",
        "\n",
        "This was due to the **vanishing gradient problem**. During backpropagation, the gradient signal has to travel backward through all the layers. In a very deep network, this signal can become exponentially smaller, until it is effectively zero by the time it reaches the early layers. As a result, these early layers stop learning.\n",
        "\n",
        "The **ResNet (Residual Network)** paper introduced a brilliantly simple solution: the **residual connection**, also known as a **skip connection**.\n",
        "\n",
        "<br>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/ba/ResBlock.png\" alt=\"Residual Connection Diagram\" width=\"500\"/>\n",
        "<br>\n",
        "\n",
        "Instead of forcing a block of layers to learn a direct mapping from an input $x$ to an output $H(x)$, a residual connection allows the block to learn a *residual function* $F(x) = H(x) - x$. The final output is then $H(x) = F(x) + x$.\n",
        "\n",
        "This seemingly minor change has a profound effect: it creates a \"shortcut\" for the gradient to flow directly through the network. If a block of layers isn't useful, the network can easily learn to make $F(x)$ zero, effectively \"skipping\" the block by passing the input $x$ through unchanged. This makes it much easier to train extremely deep networks (some ResNets have over 150 layers!).\n",
        "\n",
        "***\n",
        "\n",
        "## **4. Building a Residual Block from Scratch**\n",
        "\n",
        "Let's implement the core building block of a ResNet. A basic residual block consists of:\n",
        "*   Two convolutional layers with Batch Normalization and ReLU activations.\n",
        "*   A skip connection that adds the input of the block to the output.\n",
        "*   A special \"downsample\" connection to handle cases where the input and output dimensions don't match (e.g., when we reduce the spatial size or increase the number of channels).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af7f8406-72a8-4842-96eb-39230230e515",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af7f8406-72a8-4842-96eb-39230230e515",
        "outputId": "22bddd3b-4985-4961-a008-764eb8a9ed49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape (same dim): torch.Size([4, 64, 32, 32])\n",
            "Output shape (changed dim): torch.Size([4, 128, 16, 16])\n"
          ]
        }
      ],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        # The main path\n",
        "        # TODO: 1) 3×3 conv (in -> out) with given stride\n",
        "        # HINT: 3×3 conv → padding = 1\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels,\n",
        "                               kernel_size=___, stride=___, padding=___, bias=False)\n",
        "        # HINT: BatchNorm receives number of output channels\n",
        "        self.bn1 = nn.BatchNorm2d(...)\n",
        "\n",
        "        # HINT: same as conv1 but out→out\n",
        "        self.conv2 = nn.Conv2d(, , kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # The skip connection path\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            # If dimensions change, we need to project the input to match the output dimensions\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=..., stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Main path\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        # Add the skip connection\n",
        "        out += self....(x)\n",
        "\n",
        "        # Apply relu after the addition\n",
        "        out = F....(out)\n",
        "        return out\n",
        "\n",
        "# --- Test the block ---\n",
        "# Case 1: Dimensions stay the same\n",
        "block_same_dim = ResidualBlock(in_channels=64, out_channels=64, stride=1).to(device)\n",
        "input_tensor_1 = torch.randn(4, 64, 32, 32).to(device) # Batch of 4, 64 channels, 32x32\n",
        "output_tensor_1 = block_same_dim(input_tensor_1)\n",
        "print(f\"Output shape (same dim): {output_tensor_1.shape}\")\n",
        "\n",
        "# Case 2: Dimensions change (stride=2 halves the size, channels double)\n",
        "block_change_dim = ResidualBlock(in_channels=64, out_channels=128, stride=).to(device)\n",
        "input_tensor_2 = torch.randn(4, 64, 32, 32).to(device)\n",
        "output_tensor_2 = block_change_dim(input_tensor_2)\n",
        "print(f\"Output shape (changed dim): {...}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "765ce633-87e5-427e-8ede-231327c60156",
      "metadata": {
        "id": "765ce633-87e5-427e-8ede-231327c60156"
      },
      "source": [
        "***\n",
        "\n",
        "## **5. The Full ResNet Architecture**\n",
        "\n",
        "A full ResNet model is built by stacking these residual blocks. `torchvision` provides pre-built, highly optimized implementations of famous architectures like ResNet18, ResNet34, and ResNet50.\n",
        "\n",
        "Let's load a **ResNet18** and adapt it for our 1-channel grayscale PneumoniaMNIST dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aafccde-cd70-4e6a-916e-65bd9fe4e4e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aafccde-cd70-4e6a-916e-65bd9fe4e4e7",
        "outputId": "393a616b-23d6-4c1e-9d3e-148160f9a79a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Adapted ResNet18 Architecture ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.17M/4.17M [00:01<00:00, 3.40MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input shape: torch.Size([4, 1, 28, 28])\n",
            "Output shape: torch.Size([4, 2])\n"
          ]
        }
      ],
      "source": [
        "# Load the ResNet18 architecture\n",
        "# `pretrained=False` means we get the architecture with randomly initialized weights.\n",
        "resnet_model = torchvision.models.resnet18(weights=False)\n",
        "\n",
        "# --- Adapt the model for our specific problem ---\n",
        "# Original ResNet is designed for 3-channel (RGB) images. We need to change the first convolutional layer\n",
        "# to accept 1-channel (grayscale) images.\n",
        "# Hint: Replace the first convolution so it accepts 1 channel instead of 3\n",
        "resnet_model.conv1 = nn.Conv2d(_____, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "\n",
        "# The original model was trained on ImageNet (1000 classes). We need to change the final fully\n",
        "# connected layer to output 2 classes for our pneumonia detection task.\n",
        "num_ftrs = resnet_model.fc.in_features\n",
        "resnet_model.fc = nn.Linear(num_ftrs, )\n",
        "\n",
        "# Move the model to the device\n",
        "resnet_model.to(device)\n",
        "\n",
        "print(\"--- Adapted ResNet18 Architecture ---\")\n",
        "# print(resnet_model) # Uncomment to see the full architecture\n",
        "\n",
        "# Let's test it with a sample batch (4)\n",
        "images, _ = next(iter(DataLoader(PneumoniaMNIST(split='val', transform=transforms.ToTensor(), download=True), batch_size=)))\n",
        "# Hint: Move images to the same device as the model\n",
        "images = images.to(...)\n",
        "output = resnet_model(images)\n",
        "print(f\"\\nInput shape: {images.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfb0723d-fa1d-4f02-b5c4-f5c19fe7969b",
      "metadata": {
        "id": "cfb0723d-fa1d-4f02-b5c4-f5c19fe7969b"
      },
      "source": [
        "You can now train this ResNet model using the exact same training loop from Notebook 06! It's a drop-in replacement for our `BaselineCNN`, but far more powerful.\n",
        "\n",
        "***\n",
        "\n",
        "## **6. U-Net: The Gold Standard for Medical Image Segmentation**\n",
        "\n",
        "While ResNet is a king of **classification**, **U-Net** is the king of **segmentation**. Segmentation is the task of classifying every single pixel in an image (e.g., \"this pixel belongs to a tumor,\" \"this one belongs to healthy tissue\").\n",
        "\n",
        "The U-Net architecture is famous for its beautiful, symmetric, U-shaped structure.\n",
        "<br>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VUS2cCaPB45wcHHFp_fQZQ.png\" alt=\"U-Net Architecture Diagram\" width=\"700\"/>\n",
        "\n",
        "Source: https://short.upm.es/3udvl\n",
        "<br>\n",
        "\n",
        "It consists of two main parts:\n",
        "1.  **The Contracting Path (Encoder):** This is essentially a standard classification network (like ResNet or our baseline CNN). It takes the input image and progressively downsamples it, capturing features at different scales. The deeper it goes, the more it understands \"what\" is in the image, but it loses information about \"where.\"\n",
        "2.  **The Expansive Path (Decoder):** This path takes the low-resolution feature map from the encoder and progressively upsamples it, aiming to reconstruct a full-resolution segmentation map.\n",
        "3.  **Skip Connections:** This is the magic of U-Net. It concatenates the feature maps from the encoder at each level with the corresponding feature maps in the decoder. This allows the decoder to use both the high-level semantic information from deep in the network *and* the fine-grained spatial information from the early layers, resulting in highly precise segmentations.\n",
        "\n",
        "We will not build a full U-Net today, but it is crucial to understand that the **encoder** part of a U-Net is often a pre-trained classification network like ResNet. The features learned for classifying images are highly effective for localizing objects in segmentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e86c8c82-d9f6-4620-8678-ae9895a5b56e",
      "metadata": {
        "id": "e86c8c82-d9f6-4620-8678-ae9895a5b56e"
      },
      "source": [
        "***\n",
        "\n",
        "## **7. Summary and Next Steps**\n",
        "\n",
        "This notebook introduced you to the architectural patterns that power modern deep learning for medical imaging. You have learned:\n",
        "*   Why **Residual Connections** are so effective and how they enable the training of very deep networks.\n",
        "*   How to build a **Residual Block** from scratch.\n",
        "*   How to use a powerful, pre-built **ResNet** model from `torchvision` and adapt it to a new task.\n",
        "*   The high-level structure of **U-Net**, the standard for medical image segmentation, and its reliance on a CNN encoder and skip connections.\n",
        "\n",
        "**Challenge**: Take the adapted ResNet18 model from this notebook and train it on the PneumoniaMNIST dataset using the training loop from Notebook 06. Compare its final validation accuracy to the baseline CNN from Notebook 07. How much of an improvement do you see?\n",
        "\n",
        "In the next notebook, **`11_segmentation_unet.ipynb`**, we will build and train a full U-Net model from scratch to perform a real segmentation task.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}